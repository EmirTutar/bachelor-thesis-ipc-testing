\selectlanguage{english}
\clearpage
\addtocontents{toc}{\protect\newpage}

\section{Test Requirements for IPC Middleware}

The previous chapter explained the main concepts related to software testing, distributed systems, inter-process communication (IPC), and middleware. It also presented common middleware frameworks and discussed their differences and challenges in testing.

\vspace{1em}
To develop a structured and practical testing approach, it is important to clearly define what needs to be tested. The next chapter describes the specific requirements for testing IPC middleware. These requirements include functional aspects, such as correct message delivery, and non-functional aspects, such as timing and robustness. They form the foundation for designing a reusable and automated test system, which will be applied to eCAL in the later parts of this thesis.

\subsection{Analysis of Test Requirements}

Middleware-based systems pose unique testing requirements that differ from classical monolithic systems. Key challenges include the following:

\begin{itemize}
	\item \textbf{Timing behavior:} IPC systems are sensitive to delays, jitter, and message timing. Integration tests must capture whether messages are delivered in time and in the correct order.
	
	\item \textbf{Data consistency:} Published data must arrive at all intended subscribers with correct content. Corruption or loss during transmission must be detectable.
	
	\item \textbf{Component coordination:} Integration tests must ensure that multiple processes synchronize correctly. This includes proper startup/shutdown sequences and fault handling.
	
	\item \textbf{Fault Injection:} To evaluate the system’s robustness, integration tests should include controlled fault scenarios. These may involve simulating message loss, disabling specific network routes, or forcing a publisher to stop sending data during transmission. Such tests help verify how well the middleware handles unexpected problems.
	
	\item \textbf{Transport abstraction:} Middleware supports different transport mechanisms (e.g. TCP, shared memory, UDP). Integration tests should verify that core functionality works across transports.
\end{itemize}

\subsection{Functional and Non-Functional Test Objectives}

Integration testing in IPC middleware not only verifies the correct exchange of data between distributed components but also examines how reliably and efficiently the system performs under realistic conditions. These test objectives are generally classified into two categories: \textit{functional} and \textit{non-functional} requirements~\cite{gorton2006software, tanenbaum2017, spillner2019softwaretest}.

\vspace{1em}
\textbf{Functional Objectives}

\vspace{0.4em}
Functional tests focus on whether the system behaves correctly in terms of its specified input-output behavior. In IPC middleware, this includes the delivery of messages, handling of subscriptions, and validation of message formats~\cite{burnstein2003practical, spillner2019softwaretest}. Functional requirements describe \textit{what} the system must do, independent of the conditions under which it is executed.

\vspace{1em}
Examples of functional test objectives include:

\begin{itemize}
	\item Ensuring that a subscriber receives only messages for topics it subscribed to.
	\item Verifying that malformed messages are rejected without crashing the system.
	\item Testing that RPC requests return valid responses or trigger appropriate timeout behavior.
	\item Validating transport abstraction by checking that core communication works across TCP, UDP, and shared memory.
	\item Verifying component coordination, such as startup sequences and shutdown order, across distributed nodes.
\end{itemize}

\vspace{1em}
\textbf{Non-Functional Objectives}

\vspace{0.4em}
Non-functional tests evaluate \textit{how well} the system performs its tasks focusing on the system’s efficiency, robustness, and quality attributes. These include timing, fault tolerance, scalability, and platform independence~\cite{stallings2018, spillner2019softwaretest}.

\vspace{1em}
Key examples of non-functional test objectives:

\begin{itemize}
	\item \textbf{Timing Behavior:} Measure latency, jitter, and message ordering under various load and delay conditions.
	\item \textbf{Robustness and Recovery:} Evaluate how the system handles injected faults such as message loss, simulated crashes, or network disruptions.
	\item \textbf{Scalability and Stability:} Assess performance and resource usage as the number of processes and message rates increase.
	\item \textbf{Cross-Transport Behavior:} Ensure consistent system performance across different transport mechanisms.
\end{itemize}

According to \textit{Basiswissen Softwaretest}~\cite{spillner2019softwaretest}, non-functional requirements often affect user acceptance. Although they are sometimes assumed implicitly (e.g. reliability, maintainability), they require explicit validation.

\vspace{1em}
\textbf{Note on Benchmarking}

\vspace{0.4em}
While certain test goals such as latency and throughput measurements are commonly associated with benchmarking, this thesis does not aim to conduct a benchmarking campaign. Instead, such metrics are used selectively within integration tests to validate non-functional characteristics such as timing behavior and robustness. The primary focus lies on correctness and communication stability under defined test conditions, rather than comparative performance evaluations across systems or configurations.


\subsection{Communication Scenarios}

To create meaningful test cases, representative communication scenarios must be modeled. The following scenarios form the foundation for test design:

\begin{itemize}
	\item \textbf{One-to-one communication:} A single publisher sends messages to a single subscriber. Used to test baseline latency and correctness.
	
	\item \textbf{One-to-many communication:} A publisher sends to multiple subscribers. Tests consistency and fan-out behavior.
	
	\item \textbf{Many-to-one communication:} Multiple publishers send to a shared topic. Tests message interleaving and synchronization.
	
	\item \textbf{Multi-host communication:} Tests cross-host communication and transport compatibility.
	
	\item \textbf{Failure scenarios:} Introduce network failure, drop publishers/subscribers mid-test, or overload the system to test resilience.
\end{itemize}

\subsection{Design of a Modular Test Strategy}

To address these requirements, a modular test architecture is proposed, consisting of the following layers:

\begin{enumerate}
	\item \textbf{Test scenarios:} Each scenario defines participating nodes, timing, and expected outcomes.
	\item \textbf{Test orchestration:} A framework (e.g., Robot Framework), combined with supporting tools (e.g., Docker), manages the execution flow and the lifecycle of test components.
	\item \textbf{Test Probes:} Lightweight probes are used to monitor the communication process during testing. They capture timestamps, logs, or message payloads for later verification.
	\item \textbf{Result evaluation:} Automated scripts validate logs, output files, or metrics against expected results.
\end{enumerate}

This layered design allows reuse of components across different test cases and enables automation for continuous testing.

\subsection{Summary}

This chapter defined the requirements for integration testing in IPC middleware environments and proposed a test architecture that is both reusable and automation-friendly. In the following chapters, this test architecture will be applied to an actual middleware using eCAL as a representative implementation platform.


